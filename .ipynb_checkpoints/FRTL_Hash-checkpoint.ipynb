{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "from math import exp, copysign, log, sqrt\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "class FM_FTRL_machine(object):\n",
    "    \n",
    "    def __init__(self, fm_dim, fm_initDev, L1, L2, L1_fm, L2_fm, D, alpha, beta, alpha_fm = .1, beta_fm = 1.0, dropoutRate = 1.0):\n",
    "        ''' initialize the factorization machine.'''\n",
    "        \n",
    "        self.alpha = alpha              # learning rate parameter alpha\n",
    "        self.beta = beta                # learning rate parameter beta\n",
    "        self.L1 = L1                    # L1 regularizer for first order terms\n",
    "        self.L2 = L2                    # L2 regularizer for first order terms\n",
    "        self.alpha_fm = alpha_fm        # learning rate parameter alpha for factorization machine\n",
    "        self.beta_fm = beta_fm          # learning rate parameter beta for factorization machine\n",
    "        self.L1_fm = L1_fm              # L1 regularizer for factorization machine weights. Only use L1 after one epoch of training, because small initializations are needed for gradient.\n",
    "        self.L2_fm = L2_fm              # L2 regularizer for factorization machine weights.\n",
    "        self.fm_dim = fm_dim            # dimension of factorization.\n",
    "        self.fm_initDev = fm_initDev    # standard deviation for random intitialization of factorization weights.\n",
    "        self.dropoutRate = dropoutRate  # dropout rate (which is actually the inclusion rate), i.e. dropoutRate = .8 indicates a probability of .2 of dropping out a feature.\n",
    "        \n",
    "        self.D = D\n",
    "        \n",
    "        # model\n",
    "        # n: squared sum of past gradients\n",
    "        # z: weights\n",
    "        # w: lazy weights\n",
    "        \n",
    "        # let index 0 be bias term to avoid collisions.\n",
    "        self.n = [0.] * (D + 1) \n",
    "        self.z = [0.] * (D + 1)\n",
    "        self.w = [0.] * (D + 1)\n",
    "        \n",
    "        self.n_fm = {}\n",
    "        self.z_fm = {}\n",
    "        self.w_fm = {}\n",
    "    \n",
    "        \n",
    "    def init_fm(self, i):\n",
    "        ''' initialize the factorization weight vector for variable i.\n",
    "        '''\n",
    "        if i not in self.n_fm:\n",
    "            self.n_fm[i] = [0.] * self.fm_dim\n",
    "            self.w_fm[i] = [0.] * self.fm_dim\n",
    "            self.z_fm[i] = [0.] * self.fm_dim\n",
    "            \n",
    "            for k in range(self.fm_dim): \n",
    "                self.z_fm[i][k] = random.gauss(0., self.fm_initDev)\n",
    "    \n",
    "    def predict_raw(self, x):\n",
    "        ''' predict the raw score prior to logit transformation.\n",
    "        '''\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "        L1 = self.L1\n",
    "        L2 = self.L2\n",
    "        alpha_fm = self.alpha_fm\n",
    "        beta_fm = self.beta_fm\n",
    "        L1_fm = self.L1_fm\n",
    "        L2_fm = self.L2_fm\n",
    "        \n",
    "        # first order weights model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = self.w\n",
    "        \n",
    "        # FM interaction model\n",
    "        n_fm = self.n_fm\n",
    "        z_fm = self.z_fm\n",
    "        w_fm = self.w_fm\n",
    "        \n",
    "        raw_y = 0.\n",
    "        \n",
    "        # calculate the bias contribution\n",
    "        for i in [0]:\n",
    "            # no regularization for bias\n",
    "            w[i] = (- z[i]) / ((beta + sqrt(n[i])) / alpha)\n",
    "            \n",
    "            raw_y += w[i]\n",
    "        \n",
    "        # calculate the first order contribution.\n",
    "        for i in x:\n",
    "            sign = -1. if z[i] < 0. else 1. # get sign of z[i]\n",
    "            \n",
    "            if sign * z[i] <= L1:\n",
    "                w[i] = 0.\n",
    "            else:\n",
    "                w[i] = (sign * L1 - z[i]) / ((beta + sqrt(n[i])) / alpha + L2)\n",
    "            \n",
    "            raw_y += w[i]\n",
    "        \n",
    "        len_x = len(x)\n",
    "        # calculate factorization machine contribution.\n",
    "        for i in x:\n",
    "            self.init_fm(i)\n",
    "            for k in range(self.fm_dim):\n",
    "                sign = -1. if z_fm[i][k] < 0. else 1.   # get the sign of z_fm[i][k]\n",
    "                \n",
    "                if sign * z_fm[i][k] <= L1_fm:\n",
    "                    w_fm[i][k] = 0.\n",
    "                else:\n",
    "                    w_fm[i][k] = (sign * L1_fm - z_fm[i][k]) / ((beta_fm + sqrt(n_fm[i][k])) / alpha_fm + L2_fm)\n",
    "        \n",
    "        for i in range(len_x):\n",
    "            for j in range(i + 1, len_x):\n",
    "                for k in range(self.fm_dim):\n",
    "                    raw_y += w_fm[x[i]][k] * w_fm[x[j]][k]\n",
    "        \n",
    "        return raw_y\n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' predict the logit\n",
    "        '''\n",
    "        return 1. / (1. + exp(- max(min(self.predict_raw(x), 35.), -35.)))\n",
    "    \n",
    "    def dropout(self, x):\n",
    "        ''' dropout variables in list x\n",
    "        '''\n",
    "        for i, var in enumerate(x):\n",
    "            if random.random() > self.dropoutRate:\n",
    "                del x[i]\n",
    "    \n",
    "    def dropoutThenPredict(self, x):\n",
    "        ''' first dropout some variables and then predict the logit using the dropped out data.\n",
    "        '''\n",
    "        self.dropout(x)\n",
    "        return self.predict(x)\n",
    "    \n",
    "    def predictWithDroppedOutModel(self, x):\n",
    "        ''' predict using all data, using a model trained with dropout.\n",
    "        '''\n",
    "        return 1. / (1. + exp(- max(min(self.predict_raw(x) * self.dropoutRate, 35.), -35.)))\n",
    "    \n",
    "    def update(self, x, p, y):\n",
    "        ''' Update the parameters using FTRL (Follow the Regularized Leader)\n",
    "        '''\n",
    "        alpha = self.alpha\n",
    "        alpha_fm = self.alpha_fm\n",
    "        \n",
    "        # model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = self.w\n",
    "        \n",
    "        # FM model\n",
    "        n_fm = self.n_fm\n",
    "        z_fm = self.z_fm\n",
    "        w_fm = self.w_fm\n",
    "        \n",
    "        # cost gradient with respect to raw prediction.\n",
    "        g = p - y\n",
    "        \n",
    "        fm_sum = {}      # sums for calculating gradients for FM.\n",
    "        len_x = len(x)\n",
    "        \n",
    "        for i in x + [0]:\n",
    "            # update the first order weights.\n",
    "            sigma = (sqrt(n[i] + g * g) - sqrt(n[i])) / alpha\n",
    "            z[i] += g - sigma * w[i]\n",
    "            n[i] += g * g\n",
    "            \n",
    "            # initialize the sum of the FM interaction weights.\n",
    "            fm_sum[i] = [0.] * self.fm_dim\n",
    "        \n",
    "        # sum the gradients for FM interaction weights.\n",
    "        for i in range(len_x):\n",
    "            for j in range(len_x):\n",
    "                if i != j:\n",
    "                    for k in range(self.fm_dim):\n",
    "                        fm_sum[x[i]][k] += w_fm[x[j]][k]\n",
    "        \n",
    "        for i in x:\n",
    "            for k in range(self.fm_dim):\n",
    "                g_fm = g * fm_sum[i][k]\n",
    "                sigma = (sqrt(n_fm[i][k] + g_fm * g_fm) - sqrt(n_fm[i][k])) / alpha_fm\n",
    "                z_fm[i][k] += g_fm - sigma * w_fm[i][k]\n",
    "                n_fm[i][k] += g_fm * g_fm\n",
    "    \n",
    "    def write_w(self, filePath):\n",
    "        ''' write out the first order weights w to a file.\n",
    "        '''\n",
    "        with open(filePath, \"w\") as f_out:\n",
    "            for i, w in enumerate(self.w):\n",
    "                f_out.write(\"%i,%f\\n\" % (i, w))\n",
    "    \n",
    "    def write_w_fm(self, filePath):\n",
    "        ''' write out the factorization machine weights to a file.\n",
    "        '''\n",
    "        with open(filePath, \"w\") as f_out:\n",
    "            for k, w_fm in self.w_fm.iteritems():\n",
    "                f_out.write(\"%i,%s\\n\" % (k, \",\".join([str(w) for w in w_fm])))\n",
    "\n",
    "\n",
    "def logLoss(p, y):\n",
    "    ''' \n",
    "    calculate the log loss cost\n",
    "    p: prediction [0, 1]\n",
    "    y: actual value {0, 1}\n",
    "    '''\n",
    "    p = max(min(p, 1. - 1e-15), 1e-15)\n",
    "    return - log(p) if y == 1. else -log(1. - p)\n",
    "\n",
    "def data(filePath, hashSize, hashSalt):\n",
    "    ''' generator for data using hash trick\n",
    "    \n",
    "    INPUT:\n",
    "        filePath\n",
    "        hashSize\n",
    "        hashSalt: String with which to salt the hash function\n",
    "    '''\n",
    "    \n",
    "    for t, row in enumerate(DictReader(open(filePath))):\n",
    "        ID = row['id']\n",
    "        del row['id']\n",
    "        \n",
    "        y = 0.\n",
    "        if 'click' in row:\n",
    "            if row['click'] == '1':\n",
    "                y = 1.\n",
    "            del row['click']\n",
    "        \n",
    "        date = int(row['hour'][4:6])\n",
    "        \n",
    "        row['hour'] = row['hour'][6:]\n",
    "        \n",
    "        x = []\n",
    "        \n",
    "        for key in row:\n",
    "            value = row[key]\n",
    "            \n",
    "            index = abs(hash(hashSalt + key + '_' + value)) % hashSize + 1      # 1 is added to hash index because I want 0 to indicate the bias term.\n",
    "            x.append(index)\n",
    "        \n",
    "        yield t, date, ID, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dim = 10 \n",
    "A = [0.]*num_dim\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
